
model:
  _target_: src.models.Detector.Detector
  cfg:
    name: RetinaNet
    decision_thres: 0.5
    lr: ${datamodule.cfg.lr}
    batch_size: ${datamodule.cfg.batch_size}
    sched_factor: 0.5
    sched_patience: 4
    Chexpert_weights: False
    optim : Adam
    multiCrop: False
    combineCrops: 'mean'
    cropAttention: False
    pretrained: False
    pretrained_backbone: False
    replace_head: True
    num_classes: 2
    box_nms_thresh: 0.2
    weight_decay: 1e-5
    LR_warmup: False
    LR_Scheduler: custom

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
    monitor: "val/Competition_metric/"  #"val/mean_auc"
    save_top_k: 0
    auto_insert_metric_name: False
    save_last: True
    mode: "max"
    dirpath: "checkpoints/"
    filename: "epoch-{epoch}_step-{step}_loss-{1/val/Competition_metric/:.2f}"

  SWA:
    _target_: pytorch_lightning.callbacks.stochastic_weight_avg.StochasticWeightAveraging
    swa_epoch_start: 45 
    annealing_epochs: 15
    annealing_strategy: 'cos'

datamodule:
  _target_: src.datamodules.Datamodules.Nodule21

  cfg:
    name: Nodule21
    model: ${model.cfg.name}
    path:

      train:
        images: 
        labels: 
  
      val:  ## part of train set
        images: 
        labels: 


    sample_set: False
    preload : False

    imbalancedSampling: True
    imbalancedSamplingTest: True
    new_shape: 1024   ##length of the square image
    replacement: True # TBI
    num_gpus : 1
    batch_size: 8
    num_workers: 2
    lr : 1e-4
    test: False
    colorJitter: False
    brightness: 0.05 # colorjitter brightness
    contrast: 0
    saturation: 0
    hue: 0
    randomCrop: False
    cutout: False
    cropsize: 900
    invertIntensity: False
    vinDR_augment: True
    # class_weighting: True
    effdet: False
    all_sets_mixed: True # use v3 split!
    final_set: True

    cropStrat: downsample 

trainer: 
  _target_: pytorch_lightning.Trainer

  gpus: [0] # -1
  min_epochs: 1
  max_epochs: 60 # 60
  # limit_train_batches: 0.5 # remove when done
  gradient_clip_val: 3.0
  # limit_train_batches: 0.05
  # accelerator: ddp
  sync_batchnorm: True
  log_every_n_steps: 50
  weights_summary: null
  progress_bar_refresh_rate: 25
  # resume_from_checkpoint: null
  # profiler : "simple"
  # val_check_interval: 0
  # overfit_batches : 0.001
  precision : 16
  num_sanity_val_steps : 0 # This does not work with dp, only with ddp
  # val_check_interval: 1
  check_val_every_n_epoch: 10
  benchmark: False
  deterministic: False
  replace_sampler_ddp: True

  

