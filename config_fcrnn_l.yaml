


model:
  _target_: src.models.Detector.Detector
  cfg:
    name: FasterCNN
    decision_thres: 0.5
    lr: ${datamodule.cfg.lr}
    LR_Scheduler: None
    sched_factor: 0.5
    sched_patience: 4
    LR_warmup: True
    warmup_steps: 1000
    batch_size: ${datamodule.cfg.batch_size}
    # loss_reduction : 'none'
    optim : Adam
    # class_weighting: ${datamodule.cfg.class_weighting}
    multiCrop: False
    combineCrops: 'mean'
    cropAttention: False
    pretrained: False
    xray_weights: False
    Chexpert_weights: False
    pretrained_backbone: False
    replace_head: True
    num_classes: 2
    box_nms_thresh: 0.2
    box_score_thresh: 0.5
    trainable_layers : 5
    smoothing_epsilon: 0.1
    label_smoothing: False
    class_weighting: False
    vindr_path: /opt/algorithm/fastercnn50.pth
    vindr_weights: True

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
    monitor: "val/Competition_metric/"  #"val/mean_auc"
    save_top_k: 0
    auto_insert_metric_name: False
    save_last: True
    mode: "max"
    # prefix: ''
    dirpath: "checkpoints/"
    filename: "epoch-{epoch}_step-{step}_loss-{1/val/Competition_metric/:.2f}"
    #period: 1

  SWA:
    _target_: pytorch_lightning.callbacks.stochastic_weight_avg.StochasticWeightAveraging
    swa_epoch_start: 15 # fix the number of Epochs! to 20 here 
    # swa_lrs: NONE
    annealing_epochs: 5
    annealing_strategy: 'cos'

datamodule:
# Configfile for the NIH8 DataSet
  _target_: src.datamodules.Datamodules.Nodule21

  cfg:
    name: Nodule21
    model: ${model.cfg.name}
    path:

      train:
        images: 
        labels: 
          # - ${data_dir}/proccessed_data/splits/nodule_train_fold0.csv
          # - ${data_dir}/proccessed_data/splits/nodule_train_fold1.csv
          # - ${data_dir}/proccessed_data/splits/nodule_train_fold2.csv
          # - ${data_dir}/proccessed_data/splits/nodule_train_fold3.csv
          # - ${data_dir}/proccessed_data/splits/nodule_train_fold4.csv
        # cluster: 
        #   - ${data_dir}/proccessed_data/splits/cluster.csv
      # test: ## use the sampled test set as final test set
      #   images: ${data_dir}/
      #   labels: ${data_dir}/proccessed_data/splits/nodule_test.csv
        
  
      val:  ## part of train set
        images: 
        labels: 
          # - ${data_dir}/proccessed_data/splits/nodule_val_fold0.csv
          # - ${data_dir}/proccessed_data/splits/nodule_val_fold1.csv
          # - ${data_dir}/proccessed_data/splits/nodule_val_fold2.csv
          # - ${data_dir}/proccessed_data/splits/nodule_val_fold3.csv
          # - ${data_dir}/proccessed_data/splits/nodule_val_fold4.csv

    sample_set: False
    preload : False

    imbalancedSampling: True
    imbalancedSamplingTest: True
    new_shape: 1024   ##length of the square image
    replacement: True # TBI
    num_gpus : 1
    batch_size: 8
    num_workers: 2
    lr : 0.001
    test: False
    colorJitter: False
    brightness: 0.05 # colorjitter brightness
    contrast: 0
    saturation: 0
    hue: 0
    randomCrop: False
    cutout: False
    cropsize: 900
    invertIntensity: False
    vinDR_augment: True
    # class_weighting: True
    effdet: False
    all_sets_mixed: True # use v3 split!
    final_set: True
    # Cropping / MIL
    
    cropStrat: downsample # single_resized_crop multi_crop downsampling  
    # num_crops: 5
    # shape_before_crop: 800 
    # shape_before_resize: 0.8  # single_resized_crop

trainer: 
  _target_: pytorch_lightning.Trainer

  # set `1` to train on GPU, `0` to train on CPU only
  #gpus: 2

  gpus: [0] # -1
  min_epochs: 1
  max_epochs: 20
  gradient_clip_val: 3.0
  # accelerator: ddp
  sync_batchnorm: True
  log_every_n_steps: 50
  weights_summary: null
  progress_bar_refresh_rate: 25
  # resume_from_checkpoint: null
  # profiler : "simple"
  # val_check_interval: 0
  # overfit_batches : 0.001
  precision : 16
  num_sanity_val_steps : 0 # This does not work with dp, only with ddp
  # val_check_interval: 1
  check_val_every_n_epoch: 10
  benchmark: True
  deterministic: False
  replace_sampler_ddp: True
  # resume_from_checkpoint : "/home-alt/satish/logs/runs/DenseNet_Chexpert_competition_ExpName2021-10-21/23-11-46/checkpoints/last.ckpt" # epoch=2-step=53.ckpt"
